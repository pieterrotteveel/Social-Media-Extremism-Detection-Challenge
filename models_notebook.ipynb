{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e3bd3d",
   "metadata": {},
   "source": [
    "# 1. Distlibert Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a7774",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db43b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed51b4",
   "metadata": {},
   "source": [
    "### 1.2 Configuration (Optimized for Stability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c061dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\" \n",
    "MAX_LEN = 64      \n",
    "BATCH_SIZE = 8    \n",
    "EPOCHS = 3       \n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa84b1c",
   "metadata": {},
   "source": [
    "### 1.3 Load and Fix Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fd1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preprocessing data...\")\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "train_df.columns = ['ID', 'Original_Message', 'Extremism_Label']\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "train_df['Original_Message'] = train_df['Original_Message'].fillna(\"\").astype(str)\n",
    "test_df['Original_Message'] = test_df['Original_Message'].fillna(\"\").astype(str)\n",
    "\n",
    "label_map = {'NON_EXTREMIST': 0, 'EXTREMIST': 1}\n",
    "train_df['labels'] = train_df['Extremism_Label'].map(label_map)\n",
    "\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, stratify=train_df['labels'], random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Validation size:\", len(val_data))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "print(\"Data loaded and preprocessed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efd8cb",
   "metadata": {},
   "source": [
    "### 1.4 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "391e4439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2025/2025 [00:00<00:00, 5010.86 examples/s]\n",
      "Map: 100%|██████████| 225/225 [00:00<00:00, 3054.67 examples/s]\n",
      "Map: 100%|██████████| 750/750 [00:00<00:00, 2774.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"Original_Message\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "cols_to_remove = [\"ID\", \"Original_Message\", \"Extremism_Label\", \"__index_level_0__\"]\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True, remove_columns=cols_to_remove)\n",
    "val_tokenized = val_dataset.map(tokenize_function, batched=True, remove_columns=cols_to_remove)\n",
    "test_tokenized = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"ID\", \"Original_Message\"])\n",
    "\n",
    "train_tokenized.set_format(\"torch\")\n",
    "val_tokenized.set_format(\"torch\")\n",
    "test_tokenized.set_format(\"torch\")\n",
    "\n",
    "train_loader = DataLoader(train_tokenized, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_tokenized, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_tokenized, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ee41d",
   "metadata": {},
   "source": [
    "### 1.5 Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152b9305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "model.to(DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ec3e2",
   "metadata": {},
   "source": [
    "### 1.6 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eb42485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 1 | Loss: 0.4785\n",
      "Epoch 2 | Loss: 0.2934\n",
      "Epoch 3 | Loss: 0.1830\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69333ece",
   "metadata": {},
   "source": [
    "### 1.7 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3caa79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea445e02",
   "metadata": {},
   "source": [
    "### 1.8 Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16163c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_distilbert.csv\n"
     ]
    }
   ],
   "source": [
    "inv_map = {0: 'NON_EXTREMIST', 1: 'EXTREMIST'}\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'Extremism_Label': [inv_map[p] for p in all_preds]\n",
    "})\n",
    "\n",
    "submission.to_csv('results/submission_distilbert.csv', index=False)\n",
    "print(\"Saved submission_distilbert.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3687964",
   "metadata": {},
   "source": [
    "# 2. Twitter-RoBERTa Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de10cf",
   "metadata": {},
   "source": [
    "### 2.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c31f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b382f",
   "metadata": {},
   "source": [
    "### 2.2 Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfeabbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-offensive\"\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 6\n",
    "LEARNING_RATE = 2e-5\n",
    "CONFIDENCE_THRESHOLD = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51386cd2",
   "metadata": {},
   "source": [
    "### 2.3 GPU Detection and Memory Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5850c2",
   "metadata": {},
   "source": [
    "### 2.4 Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bff32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ef78c",
   "metadata": {},
   "source": [
    "### 2.5 Load and Fix Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3741b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading data...\")\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "train_df.columns = ['ID', 'Original_Message', 'Extremism_Label']\n",
    "\n",
    "train_df['Original_Message'] = train_df['Original_Message'].fillna(\"\").astype(str)\n",
    "test_df['Original_Message'] = test_df['Original_Message'].fillna(\"\").astype(str)\n",
    "\n",
    "label_map = {'NON_EXTREMIST': 0, 'EXTREMIST': 1}\n",
    "train_df['labels'] = train_df['Extremism_Label'].map(label_map)\n",
    "\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.15, stratify=train_df['labels'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd89b6b5",
   "metadata": {},
   "source": [
    "### 2.6 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading Tokenizer ({MODEL_NAME})...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Tokenizing data...\")\n",
    "train_encodings = tokenizer(train_data['Original_Message'].tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n",
    "val_encodings = tokenizer(val_data['Original_Message'].tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n",
    "test_encodings = tokenizer(test_df['Original_Message'].tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n",
    "\n",
    "print(\"Creating datasets and dataloaders...\")\n",
    "train_dataset = SimpleDataset(train_encodings, train_data['labels'].tolist())\n",
    "val_dataset = SimpleDataset(val_encodings, val_data['labels'].tolist())\n",
    "test_dataset = SimpleDataset(test_encodings)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c0b4f",
   "metadata": {},
   "source": [
    "### 2.7 Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff82f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "model.to(DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573726b6",
   "metadata": {},
   "source": [
    "### 2.8 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bae397",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % 20 == 0 and step > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Step {step}/{len(train_loader)} | Loss: {loss.item():.4f} | Time: {elapsed:.0f}s\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b41dc",
   "metadata": {},
   "source": [
    "### 2.9 Precision Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4452f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nChecking Precision on Validation Set...\")\n",
    "model.eval()\n",
    "val_probs = []\n",
    "val_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "        val_probs.extend(probs[:, 1].cpu().numpy())\n",
    "        val_true.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "val_preds_strict = [1 if p > CONFIDENCE_THRESHOLD else 0 for p in val_probs]\n",
    "\n",
    "target_names = ['NON_EXTREMIST', 'EXTREMIST']\n",
    "print(f\"\\nResults with {CONFIDENCE_THRESHOLD*100}% Confidence Threshold\")\n",
    "print(classification_report(val_true, val_preds_strict, target_names=target_names))\n",
    "\n",
    "cm = confusion_matrix(val_true, val_preds_strict)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title(f'Precision Matrix (Threshold {CONFIDENCE_THRESHOLD})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467eb851",
   "metadata": {},
   "source": [
    "### 2.10 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPredicting on Test Set...\")\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "        test_probs.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "test_preds = [1 if p > CONFIDENCE_THRESHOLD else 0 for p in test_probs]\n",
    "\n",
    "inv_map = {0: 'NON_EXTREMIST', 1: 'EXTREMIST'}\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'Extremism_Label': [inv_map[p] for p in test_preds]\n",
    "})\n",
    "\n",
    "submission.to_csv('results/submission_roberta_precision.csv', index=False)\n",
    "print(\"Saved 'submission_roberta_precision.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
